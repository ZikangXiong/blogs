{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thirty-anime",
   "metadata": {},
   "source": [
    "# Hierarchical RL\n",
    "\n",
    "[Learning Multi-Level Hierarchy with Hindsight](#Learning-Multi-Level-Hierarchy-with-Hindsight)  \n",
    "[Hindsight Experience Replay](#Hindsight-Experience-Replay)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-smooth",
   "metadata": {},
   "source": [
    "## Learning Multi-Level Hierarchy with Hindsight \n",
    "\n",
    "[Link](https://arxiv.org/pdf/1712.00948.pdf), [Code](https://github.com/nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch), [Open Review](https://openreview.net/forum?id=ryzECoAcY7)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-venice",
   "metadata": {},
   "source": [
    "This work combines the universal value function approximator (UVFA) and Hindsight Experience Replay (HER).  \n",
    "UVFA will be used to estimate the action-value function of a goal-conditioned policy $\\pi, q_\\pi(s, g, a)$.   \n",
    "HER is a data augmentation technique that can accelerate learning in sparse reward tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-throat",
   "metadata": {},
   "source": [
    "### Technique Details\n",
    "#### Space\n",
    "The state spaces of all layers of the hierarchy is identical.  \n",
    "The action space of bottom-most layer is identical to the original action space of the task, other layer has the space that is identical to the state space. \n",
    "\n",
    "#### Nested Policy\n",
    "Policy at layer $i$ generate the goal of layer $i-1$.  \n",
    "\n",
    "#### Hindsight Action Transitions\n",
    "Similar to [Hindsight Experience Replay](#Hindsight-Experience-Replay).  \n",
    "\n",
    "#### Subgoal Test Transitions\n",
    "To my understanding, this is just a simulation see whether the hindsight goal can be achieved or not, but their claim and explanation is not very clear to me. More specifically, I did not see how the subgoal test solves the problem that the subgoal cannot be achieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-correlation",
   "metadata": {},
   "source": [
    "### Code\n",
    "Reading note of [this](https://github.com/nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch) implementation.  \n",
    "#### Check Goal\n",
    "The if the agent is close enough to the goal, the goal is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_goal(self, state, goal, threshold):\n",
    "    for i in range(self.state_dim):\n",
    "        if abs(state[i] - goal[i]) > threshold[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-edinburgh",
   "metadata": {},
   "source": [
    "#### Hindsight Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hindsight action transition\n",
    "if goal_achieved:\n",
    "    self.replay_buffer[i_level].add((state, action, 0.0, next_state, goal, 0.0, float(done)))\n",
    "else:\n",
    "    # If the agent does not achieve the goal, it will be penelized. \n",
    "    self.replay_buffer[i_level].add((state, action, -1.0, next_state, goal, self.gamma, float(done)))\n",
    "                        \n",
    "# hindsight goal transition\n",
    "# last transition reward and discount is 0\n",
    "goal_transitions[-1][2] = 0.0\n",
    "goal_transitions[-1][5] = 0.0\n",
    "for transition in goal_transitions:\n",
    "    # last state is goal for all transitions\n",
    "    transition[4] = next_state\n",
    "    self.replay_buffer[i_level].add(tuple(transition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-psychiatry",
   "metadata": {},
   "source": [
    "#### Subgoal Test\n",
    "There is some possibility to enable the subgoal test. When it is enabled, no exploration noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is a subgoal test, then next/lower level goal has to be a subgoal test\n",
    "is_next_subgoal_test = is_subgoal_test\n",
    "\n",
    "# Determine whether to test subgoal (action)\n",
    "if np.random.random_sample() < self.lamda:\n",
    "    is_next_subgoal_test = True\n",
    "\n",
    "# add noise or take random action if not subgoal testing\n",
    "if not is_subgoal_test:\n",
    "    if np.random.random_sample() > 0.2:\n",
    "        action = action + np.random.normal(0, self.exploration_action_noise)\n",
    "        action = action.clip(self.action_clip_low, self.action_clip_high)\n",
    "    else:\n",
    "        action = np.random.uniform(self.action_clip_low, self.action_clip_high)\n",
    "\n",
    "# If subgoal was tested but not achieved, add subgoal testing transition.\n",
    "# The -self.H is the penatly. Discount factor is set to 0\n",
    "if is_next_subgoal_test and not self.check_goal(action, next_state, self.threshold):\n",
    "    self.replay_buffer[i_level].add((state, action, -self.H, next_state, goal, 0.0, float(done)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-substance",
   "metadata": {},
   "source": [
    "### Reviews\n",
    "765 can get this paper in, but is not a very promising score. Main complains are \n",
    "1. The multi-layer hierarchy is not fully explored. \n",
    "2. The subgoal test is criticized.  \n",
    "3. Benchmark is not as complicated as other HRL papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-detroit",
   "metadata": {},
   "source": [
    "## Hindsight Experience Replay\n",
    "[link](https://arxiv.org/pdf/1707.01495.pdf)  \n",
    "\n",
    "Handle the challenge caused by binary, sparse reward.  \n",
    "By replaying the experience with different goal (no additional simulation needed), more informative reward can be generated. But this approach is different with reward shaping, because it does not requires any domain knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-scratch",
   "metadata": {},
   "source": [
    "Prerequired knowledge: Universal Value Function Approximators (UVFA) and how replay buffer.   \n",
    "Read the example in **section 3.1** and **algorithm 1**, one can easily get how this simple approach works.   \n",
    "The $r_g$ is defined in the section 4.1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
