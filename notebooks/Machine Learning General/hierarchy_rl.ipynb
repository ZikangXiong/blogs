{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reliable-paradise",
   "metadata": {},
   "source": [
    "# Hierarchy RL\n",
    "\n",
    "[Learning Multi-Level Hierarchy with Hindsight](#Learning-Multi-Level-Hierarchy-with-Hindsight)  \n",
    "[Hindsight Experience Replay](#Hindsight-Experience-Replay)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-franklin",
   "metadata": {},
   "source": [
    "## Learning Multi-Level Hierarchy with Hindsight \n",
    "\n",
    "[Link](https://arxiv.org/pdf/1712.00948.pdf), [Code](https://github.com/nikhilbarhate99/Hierarchical-Actor-Critic-HAC-PyTorch), [Open Review](https://openreview.net/forum?id=ryzECoAcY7)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-disaster",
   "metadata": {},
   "source": [
    "This work combines the universal value function approximator (UVFA) and Hindsight Experience Replay (HER).  \n",
    "UVFA will be used to estimate the action-value function of a goal-conditioned policy $\\pi, q_\\pi(s, g, a)$.   \n",
    "HER is a data augmentation technique that can accelerate learning in sparse reward tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-trance",
   "metadata": {},
   "source": [
    "### Technique Details\n",
    "#### Space\n",
    "The state spaces of all layers of the hierarchy is identical.  \n",
    "The action space of bottom-most layer is identical to the original action space of the task, other layer has the space that is identical to the state space. \n",
    "\n",
    "#### Nested Policy\n",
    "Policy at layer $i$ generate the goal of layer $i-1$.  \n",
    "\n",
    "#### Hindsight Action Transitions\n",
    "Similar to [Hindsight Experience Replay](#Hindsight-Experience-Replay).  \n",
    "\n",
    "#### Subgoal Test Transitions\n",
    "To my understanding, this is just a random simulation see whether the hindsight goal can be achieved or not, but their claim and explanation is not very clear to me.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-lover",
   "metadata": {},
   "source": [
    "## Hindsight Experience Replay\n",
    "[link](https://arxiv.org/pdf/1707.01495.pdf)  \n",
    "\n",
    "Handle the challenge caused by binary, sparse reward.  \n",
    "By replaying the experience with different goal (no additional simulation needed), more informative reward can be generated. But this approach is different with reward shaping, because it does not requires any domain knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-skiing",
   "metadata": {},
   "source": [
    "Prerequired knowledge: Universal Value Function Approximators (UVFA) and how replay buffer.   \n",
    "Read the example in **section 3.1** and **algorithm 1**, one can easily get how this simple approach works.   \n",
    "The $r_g$ is defined in the section 4.1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
